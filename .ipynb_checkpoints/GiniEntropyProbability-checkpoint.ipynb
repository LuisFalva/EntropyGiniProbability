{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impureza de Gini y Entropía\n",
    "\n",
    "\n",
    "# Impureza de Gini\n",
    "<br> \n",
    "<font size=\"4\"> La impureza de Gini es una magnitud de incertidumbre, esto se refiere a la aleatoriedad con la que se elige un elemento $i$ del conjunto de datos y la probabilidad de asignarle al elemento $i$ una etiqueta $m$ de forma incorrecta. </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $Gini = 1 - \\sum_{i}^{c} (p_{i})^{2}$\n",
    "\n",
    "\n",
    "## Análisis de la función.\n",
    "<br> \n",
    "<font size=\"4\">Para cada clase se deberá calcular la probabilidad que le corresponda, esta es la probabilidad que posee cada clase de ser escogida y se representa como $p_{i}$. Estas probabilidades de cada clase se pueden calcular por separado. \n",
    "<br> \n",
    "<br> \n",
    "Por lo tanto, para calcular la impureza de Gini, crearemos primero (y por separado) una función que calcule las $p_{i}$'s, para así poder usarlas almacenando en un diccionario de Python los valores de las $k$ clases.\n",
    "<br> \n",
    "<br> \n",
    "Ahora bien, a partir de esta función que calcula las probabilidades, almacenamos en una nueva variable $P = \\sum (p_{i})^2$ que es la suma las probabilidades al cuadrado. Si se aplica $1 - P$ esto representa el _complemento_ de la probabilidad $P$ i.e. la parte _contraria de la probabilidad_. A partir de tener estos cálculos ya podemos seguir a Calcular tanto Gini como la entropía.\n",
    "<br>\n",
    "<br> \n",
    "Al $1 - P$, que obtuvimos en el paso anterior, la consideramos una maginitud contraria a la suma de las probabilidades al cuadrado $P$. $1 - P$ describe la probabilidad de ser etiquetado de forma incorrecta por el nivel de impureza en los datos, de aquí nace la idea de la _magnitud de incertidumbre_, i.e. _Gini_.</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">**Por ahora crearemos nuestros nodos los cuales almacenarán información de cada clase**</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Root_node = {\"Manzana\": 100, \"Platano\": 400, \"Mango\": 5, \"Uva\": 550}\n",
    "child_node_st = {\"Manzana\": 100, \"Platano\": 400, \"Mango\": 5}\n",
    "child_node_nd = {\"Platano\": 400, \"Mango\": 5}\n",
    "child_node_rd = {\"Platano\": 400}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Lo que se pretende mostrar con este ejercicio *dummy* es el nivel de información que aporta cada clase y el uso de las probabilidades de cada una. Pretendemos que sea un ejercicio mental básico, pero que soporta una ley que hay detrás de estas dos funciones: la ley de probabilidad.</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos de entrada: {'Manzana': 100, 'Platano': 400, 'Mango': 5, 'Uva': 550}\n",
      "Suma Total: 1055\n",
      "Prob Manzana: 0.0947867298578199\n",
      "Prob Platano: 0.3791469194312796\n",
      "Prob Mango: 0.004739336492890996\n",
      "Prob Uva: 0.5213270142180095\n",
      "\n",
      "Suma de probabilidades: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Ejercicio Dummy.\n",
    "print(\"Datos de entrada:\", Root_node)\n",
    "print(\"Suma Total:\", 100+400+5+550)\n",
    "print(\"Prob Manzana:\", 100/1055)\n",
    "print(\"Prob Platano:\", 400/1055)\n",
    "print(\"Prob Mango:\", 5/1055)\n",
    "print(\"Prob Uva:\", 550/1055)\n",
    "print(\"\\nSuma de probabilidades:\", (100/1055)+(400/1055)+(5/1055)+(550/1055))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Ok! Suficiente, codifiquemos...</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def probability_class(node):\n",
    "    node_sum = sum(node.values())\n",
    "    percents = {c:v / node_sum for c, v in node.items()}\n",
    "    return node_sum, percents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Gini requiere menos costo computacional que el cálculo de la Entropía, debido al menor número de operaciones aritméticas que realiza la función (la suma de las probabilidades al cuadrado). Por ende, a las probabilidades menores casi no las toma en cuenta y éstas tienden a influir muy poco (o casi nada) en el resultado final.</font> \n",
    "\n",
    "## _Cuenta demostrativa:_\n",
    "<br> \n",
    "<font size=\"4\"> $0.0948^{2} + 0.3791^{2} + 0.0047^{2} + 0.5213^{2}$\n",
    "<br> <br> \n",
    "$0.009 + 0.1438 + 0.00002209 + 0.2718$\n",
    "<br> <br> \n",
    "$P = 0.4246$\n",
    "<br> <br> \n",
    "$Gini = 1 - P = 0.5754$</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">La función `gini score` es la función por la que obtendremos la magnitud Gini (explicada al inicio del notebook), para ello en `gini_score` agregamosla función `probability_class` para mejorar y atomizar ambos comportamientos.</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gini_score(node):\n",
    "    _, percents = probability_class(node)\n",
    "    # donde i contiene la probabilidad calculada del nodo en cuestión\n",
    "    score = round(1 - sum([i**2 for i in percents.values()]), 3)\n",
    "    print('Gini Score for node {} : {}'.format(node, score))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropía\n",
    "\n",
    "## Análisis de la función\n",
    "<br> \n",
    "<font size=\"4\">La función de Entropía es menos eficiente en términos computacionales dado que involucra el uso de $\\log_{2}$, pero lo interesante aquí es que como usa el $\\log$ a las probabilidades menores y mayores, las ajusta y pondera las probabilidades que son pequeñas. Por ello, el resultado será mucho más sensible a estas probabilidades pequeñas que en el casro del score de Gini. En los casos que se busque darle relevancia a las probabilidades pequeñas, se recomienda usar la Entropía como medida de impureza en lugar del score de Gini.</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $E(S) = \\sum_{i}^{c} - p_{i} \\log_{2} p_{i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Cuenta demostrativa:_\n",
    "<br> \n",
    "<font size=\"4\">$- 0.0948 \\log_{2}(0.0948) + -0.3791 \\log_{2}(0.3791) + -0.0047 \\log_{2}(0.0047) + -0.5213 \\log_{2}(0.5213)$\n",
    "<br> <br> \n",
    "$0.3222 + 0.5304 + 0.0363 + 0.4899$\n",
    "<br> <br> \n",
    "$Entropy = 1.3788$</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def entropy_score(node):\n",
    "    _, percents = probability_class(node)\n",
    "    # donde i contiene la probabilidad calculada del nodo en cuestión\n",
    "    score = round(sum([-i * log(i, 2) for i in percents.values()]), 3)\n",
    "    print('Entropy Score for node {} : {}'.format(node, score))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\"> Ahora calcularemos la ganancia de información (`information_gain`), a menor impureza, mayor ganancia de información.</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def information_gain(parent, children, criterion):\n",
    "    score = {'gini': gini_score, 'entropy': entropy_score}\n",
    "    metric = score[criterion]\n",
    "    parent_score = metric(parent)\n",
    "    parent_sum = sum(parent.values())\n",
    "    weighted_child_score = sum([metric(i) * sum(i.values()) / parent_sum  for i in children])\n",
    "    gain = round((parent_score - weighted_child_score),2)\n",
    "    print('Information gain: {}'.format(gain))\n",
    "    return gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">**Aquí mostramos el funcionamiento tanto de Gini como de la Entropía. En este caso el número 5 quiere decir que estamos incluyendo solamente 5 registros que contienen la clase \"Mango\", por lo tanto podemos notar que hay un importante desbalance entre las cantidades por cada clase, por ejemplo:**</font> \n",
    "<br> <br> \n",
    "- <font size=\"4\">En el caso de Mazana, Platano y Uva, éstas contienen cantidades por encima de los 100 registros, claramente son las clases predominantes en este ejemplo, por lo que serán de gran influencia en la separación de la información en cada nodo hijo</font> \n",
    "<br> <br> \n",
    "- <font size=\"4\">En el caso de la clase Mango, podemos suponer que la probabilidad calculada para ella será muy pequeña, por lo que será muy probable que la función de magnitud Gini ignore dicha clase</font> \n",
    "<br> <br> \n",
    "- <font size=\"4\">Por otro lado vemos que la función de Entropia, no solo logra distinguir a la clase minoritaria, sino que separa a mayor precisión la información que reside en cada clase dando como resultado final una mayor ganancia de información.\n",
    "</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini Score for node {'Manzana': 100, 'Platano': 400, 'Mango': 5, 'Uva': 550} : 0.575\n",
      "Gini Score for node {'Manzana': 100, 'Platano': 400, 'Mango': 5} : 0.333\n",
      "Gini Score for node {'Platano': 400, 'Mango': 5} : 0.024\n",
      "Gini Score for node {'Platano': 400} : 0.0\n",
      "Information gain: 0.41\n"
     ]
    }
   ],
   "source": [
    "gini_gain = information_gain(parent=Root_node, children=[child_node_st, child_node_nd, child_node_rd], criterion='gini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy Score for node {'Manzana': 100, 'Platano': 400, 'Mango': 5, 'Uva': 550} : 1.379\n",
      "Entropy Score for node {'Manzana': 100, 'Platano': 400, 'Mango': 5} : 0.795\n",
      "Entropy Score for node {'Platano': 400, 'Mango': 5} : 0.096\n",
      "Entropy Score for node {'Platano': 400} : 0.0\n",
      "Information gain: 0.96\n"
     ]
    }
   ],
   "source": [
    "entropy_gain = information_gain(parent=Root_node, children=[child_node_st, child_node_nd, child_node_rd], criterion='entropy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
